name: Distributed Recursive Enumeration

on:
  workflow_dispatch: {}

permissions:
  contents: write   # For creating matrix.json and artifacts
  actions: write    # For triggering workflows in other repositories

env:
  # --- Chunking and Distribution Configuration ---
  LINES_PER_CHUNK: 15            # MODIFIED: As requested for assetfinder seeds
  DISTRIBUTION_THRESHOLD: 20      # The minimum total chunks to trigger 3-way distribution
  PRIMARY_PERCENTAGE: 40          # 45% for the primary account
  SECONDARY_PERCENTAGE: 35        # 40% for the secondary account
  
  STORE_REPO_NAME: ${{ secrets.STORE }}
  # --- Account 2 Details (for triggering and committing) ---
  ACCOUNT2_REPO_OWNER: ${{ secrets.ACCOUNT2_REPO_OWNER }}
  ACCOUNT2_REPO_NAME: ${{ secrets.ACCOUNT2_REPO_NAME  }} 
   # The 'store-recon' repo name

  # --- Account 3 Details ---
  ACCOUNT3_REPO_OWNER: ${{ secrets.ACCOUNT3_REPO_OWNER  }}
  ACCOUNT3_REPO_NAME: ${{ secrets.ACCOUNT3_REPO_NAME  }} 

jobs:
  prepare_all_chunks_and_package:
    name: Prepare Seed Chunks & Package
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    outputs:
      all_chunks_matrix_json: ${{ steps.build_full_matrix.outputs.full_matrix }}
      total_chunks_count: ${{ steps.build_full_matrix.outputs.total_chunks }}
      chunk_package_artifact_name: "assetfinder-chunks-package-${{ github.run_id }}"
    if: ${{ github.actor == 'Pcoder7' }}
    steps:
      - name: Log trigger info
        run: |
          echo "Triggered by: ${{ github.actor }}"
          echo "Run id: ${{ github.run_id }}"    
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.22'

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-

      - name: Install Tools
        run: |          
          
          if ! command -v dsieve >/dev/null; then
            echo "Installing dsieveâ€¦"
            go install github.com/trickest/dsieve@latest
          else
            echo "dsieve already in cache"
          fi    
         
          # Installing inscope
          if ! command -v anew >/dev/null; then
            echo "Installing anew"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in cache"
          fi   
            
      # STEP 1: Build the smart seed list from the store-recon repo
      - name: Build Smart Tiered & Randomized Seed List
        id: create_final_list
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
          STORE: ${{secrets.STORE}}
        run: |

          echo "INFO: Performing a fast, shallow clone of the results repository..."
          git clone --depth 1 "https://x-access-token:${{ env.STORE_RECON_PAT }}@github.com/${{ env.ACCOUNT2_REPO_OWNER }}/${{ env.STORE_REPO_NAME }}.git" store-recon-temp
          
          echo "INFO: Aggregating all results into a master list..."
          find store-recon-temp/results -type f -name "puredns_result.txt" -exec awk '{print $1}' {} + > all-resolved-subdomains.txt
          if [ ! -s all-resolved-subdomains.txt ]; then
              echo "::warning:: Master subdomain list is empty. No seeds to generate."
              touch final-subdomain-list.txt
              exit 0
          fi
          echo "INFO: Extracting unique root domains..."
          dsieve -if all-resolved-subdomains.txt -f 2 | sort -u > unique_root_domains.txt
          > final-subdomain-list.txt
          echo "INFO: Building smart seed list for each root domain..."
          while read -r ROOT_DOMAIN; do
            if [ -z "$ROOT_DOMAIN" ]; then continue; fi
            echo "====================================================="
            echo "Processing Root Domain: $ROOT_DOMAIN"
            
            TEMP_ROOT_LIST="${TMPDIR}/${ROOT_DOMAIN}-list.txt"
            grep -E "(^|\\.)${ROOT_DOMAIN//./\\.}(\$)" all-resolved-subdomains.txt > "$TEMP_ROOT_LIST" || true
            if [ ! -s "$TEMP_ROOT_LIST" ]; then continue; fi
            TEMP_SEED_LIST_DOMAIN="${TMPDIR}/${ROOT_DOMAIN}-seeds.txt"
            > "$TEMP_SEED_LIST_DOMAIN"
            
            # Tier 1: Random sample of the top-tier
            #echo "  -> Getting a random 250 sample from the top 500 subdomains..."
            #dsieve -if "$TEMP_ROOT_LIST" -f 3 -top 500 | shuf | head -n 250 | anew -q "$TEMP_SEED_LIST_DOMAIN"
            
            # Tier 2: Random sample of the mid-tier
            echo "  -> Getting a random 150 sample from the mid-tier (ranks 501-1000)..."
            dsieve -if "$TEMP_ROOT_LIST" -f 3 -top 1000 | tail -n 500 | shuf | head -n 150 | anew -q "$TEMP_SEED_LIST_DOMAIN"
            cat "$TEMP_SEED_LIST_DOMAIN" >> final-subdomain-list.txt
          done < unique_root_domains.txt
          
          echo "INFO: Performing final de-duplication of the master seed list..."
          mv final-subdomain-list.txt final-subdomain-list.tmp
          sort -u final-subdomain-list.tmp > final-subdomain-list.txt
          rm final-subdomain-list.tmp
          echo "SUCCESS: 'final-subdomain-list.txt' created with $(wc -l < final-subdomain-list.txt) total unique seeds."
          rm -rf store-recon-temp
      # STEP 2: Chunk the final seed list for distribution
      - name: Build Full Matrix & Create Chunks
        id: build_full_matrix
        shell: bash
        run: |
          JSON_MATRIX='[]'
          if [ ! -s final-subdomain-list.txt ]; then
            echo "WARNING: final-subdomain-list.txt is empty. No chunks will be generated."
          else
            echo "INFO: Creating chunk directory 'chunks/all-seeds'..."
            mkdir -p chunks/all-seeds
            
            echo "INFO: Splitting 'final-subdomain-list.txt' into chunks of $LINES_PER_CHUNK lines..."
            split -l "$LINES_PER_CHUNK" -a 4 --numeric-suffixes=1 final-subdomain-list.txt "chunks/all-seeds/chunk_"
            
            while IFS= read -r chunk_file_path; do
              JSON_MATRIX=$(printf '%s' "$JSON_MATRIX" | jq -c --arg c "$chunk_file_path" '. + [{chunk:$c}]')
            done < <(find "chunks/all-seeds/" -name 'chunk_*' -type f)
          fi
          
          TOTAL_CHUNKS=$(echo "$JSON_MATRIX" | jq 'length')
          echo "FINAL_BUILD_INFO: Total chunks in matrix: $TOTAL_CHUNKS"
          echo "$JSON_MATRIX" > full_matrix.json
          echo "full_matrix=$JSON_MATRIX" >> $GITHUB_OUTPUT
          echo "total_chunks=$TOTAL_CHUNKS" >> $GITHUB_OUTPUT
      # STEP 3: Package the chunks into an artifact
      - name: Package All Chunks
        id: package_chunks
        if: steps.build_full_matrix.outputs.total_chunks > 0
        shell: bash
        run: |
          BASE_ARTIFACT_NAME="assetfinder-chunks-package-${{ github.run_id }}"
          PACKAGE_TAR_FILENAME="${BASE_ARTIFACT_NAME}.tar.gz"
          echo "INFO: Creating tarball: $PACKAGE_TAR_FILENAME"
          tar -czvf "$PACKAGE_TAR_FILENAME" chunks full_matrix.json
          echo "package_tar_filename=$PACKAGE_TAR_FILENAME" >> $GITHUB_OUTPUT
          echo "base_artifact_name=$BASE_ARTIFACT_NAME" >> $GITHUB_OUTPUT
      # STEP 4: Upload the artifact
      - name: Upload Full Chunks Package
        if: steps.build_full_matrix.outputs.total_chunks > 0
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.package_chunks.outputs.base_artifact_name }}
          path: ${{ steps.package_chunks.outputs.package_tar_filename }}
          retention-days: 1

  # This job is identical to your reference and needs no changes
  distribute_and_trigger_secondary:
    name: Distribute Work & Trigger Workers
    needs: prepare_all_chunks_and_package
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0
    runs-on: ubuntu-latest
    outputs:
      primary_matrix_json: ${{ steps.calculate_distribution.outputs.primary_matrix }}
      secondary_matrix_json: ${{ steps.calculate_distribution.outputs.secondary_matrix }}
      tertiary_matrix_json: ${{ steps.calculate_distribution.outputs.tertiary_matrix }}
    steps:
      - name: Calculate Chunk Distribution for Accounts
        id: calculate_distribution
        shell: bash
        run: |
          ALL_CHUNKS_JSON='${{ needs.prepare_all_chunks_and_package.outputs.all_chunks_matrix_json }}'
          TOTAL_CHUNKS=${{ needs.prepare_all_chunks_and_package.outputs.total_chunks_count }}
          THRESHOLD=${{ env.DISTRIBUTION_THRESHOLD }}
          if [ "$TOTAL_CHUNKS" -lt "$THRESHOLD" ]; then
            PRIMARY_CHUNKS_JSON="$ALL_CHUNKS_JSON"
            SECONDARY_CHUNKS_JSON="[]"
            TERTIARY_CHUNKS_JSON="[]"
          else
            PRIMARY_PERCENT=${{ env.PRIMARY_PERCENTAGE }}
            SECONDARY_PERCENT=${{ env.SECONDARY_PERCENTAGE }}
            PRIMARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $PRIMARY_PERCENT) / 100" | bc)
            SECONDARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $SECONDARY_PERCENT) / 100" | bc)
            OFFSET_FOR_TERTIARY=$((PRIMARY_CHUNK_COUNT + SECONDARY_CHUNK_COUNT))
            PRIMARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson limit "$PRIMARY_CHUNK_COUNT" '.[0:$limit]')
            SECONDARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$PRIMARY_CHUNK_COUNT" --argjson limit "$SECONDARY_CHUNK_COUNT" '.[$offset : $offset+$limit]')
            TERTIARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$OFFSET_FOR_TERTIARY" '.[$offset:]')
          fi
          
          echo "primary_matrix=$PRIMARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
          echo "secondary_matrix=$SECONDARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
          echo "tertiary_matrix=$TERTIARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
      
      # The rest of this job (preparing payloads and triggering) is complex but can be copied
      # verbatim from your original workflow. Just ensure the 'workflow:' name is correct.
      # For brevity, I'll show one trigger block. Repeat for tertiary.
      
      - name: Prepare Trigger Payload For Secondary
        id: prepare_payload_secondary
        if: steps.calculate_distribution.outputs.secondary_matrix != '[]'
        shell: bash
        run: |
          MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.secondary_matrix }}'
          JSON_PAYLOAD=$(jq -cn \
            --arg server_url "${{ github.server_url }}" \
            --arg repo_owner "${{ github.repository_owner }}" \
            --arg repo_name "${{ github.event.repository.name }}" \
            --arg run_id "${{ github.run_id }}" \
            --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
            --arg matrix_as_a_string "${MATRIX_AS_STRING}" \
            '{
              "primary_github_server_url": $server_url,
              "primary_repo_owner": $repo_owner,
              "primary_repo_name": $repo_name,
              "primary_run_id": $run_id,
              "chunk_package_artifact_name": $artifact_name,
              "secondary_matrix_json": $matrix_as_a_string
            }')
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT
      
      - name: Trigger Secondary Account Workflow
        if: steps.calculate_distribution.outputs.secondary_matrix != '[]'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: recurse.yml # IMPORTANT: This must match the workflow name in the secondary repo
          repo: ${{ secrets.ACCOUNT2_REPO_OWNER }}/${{ secrets.ACCOUNT2_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          inputs: ${{ steps.prepare_payload_secondary.outputs.json_string }}
          ref: main
      
      # ... (Add identical 'Prepare' and 'Trigger' blocks for Tertiary Account) ...
      #====================================================================
      # ADD THE FIRST SLEEP STEP HERE
      # ====================================================================
      #- name: Add delay before triggering Tertiary Trigger worker
       # if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        #run: |
         # echo "Pausing for 30 seconds to space out worker triggers and avoid platform throttling..."
          #sleep 30

      - name: Add Delay Between API Triggers for Tertiary Account
        run: |
          echo "Pausing for 30 seconds to avoid API rate-limiting before triggering the Tertiary account..."
          sleep 30

      # ====================================================================
      
      - name: Prepare Trigger Payload For Tertiary
        id: prepare_payload_tertiary
        if: steps.calculate_distribution.outputs.tertiary_matrix != '[]'
        shell: bash
        run: |
          MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.tertiary_matrix }}'
          JSON_PAYLOAD=$(jq -cn \
            --arg run_id "${{ github.run_id }}" \
            '{
              "primary_run_id": $run_id
            }')
             echo "Constructed JSON Payload: $JSON_PAYLOAD"
             echo "json_string=$JSON_PAYLOAD" >> "$GITHUB_OUTPUT"
           
           JSON_PAYLOAD=$(jq -cn \
             --arg server_url "${{ github.server_url }}" \
             --arg repo_owner "${{ github.repository_owner }}" \
             --arg repo_name "${{ github.event.repository.name }}" \
             --arg run_id "${{ github.run_id }}" \
             --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
             --arg matrix_as_a_string "${MATRIX_AS_STRING}" \
             '{
               "primary_github_server_url": $server_url,
               "primary_repo_owner": $repo_owner,
               "primary_repo_name": $repo_name,
               "primary_run_id": $run_id,
               "chunk_package_artifact_name": $artifact_name,
               "tertiary_matrix_json": $matrix_as_a_string 
             }')
          
          echo "Constructed JSON Payload: $JSON_PAYLOAD"
             # Set the constructed JSON string as an output of this step
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT
      
      - name: DEBUG - Show Tertiary Inputs String
        if: steps.calculate_distribution.outputs.tertiary_matrix != '[]'
        run: |
          echo "DEBUG: Inputs for TERTIARY account:"
          # echo "${{ steps.prepare_payload_tertiary.outputs.json_string }}"    
     
      - name: Trigger Tertiary Account Workflow
        # This is a NEW step, a copy of the secondary trigger step for Account 3.
        id: trigger_tertiary
        if: steps.calculate_distribution.outputs.tertiary_matrix != '[]'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: recurse.yml
          repo: ${{ secrets.ACCOUNT3_REPO_OWNER }}/${{ secrets.ACCOUNT3_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_TERTIARY_ACCOUNT_REPO }}
          inputs: ${{ steps.prepare_payload_tertiary.outputs.json_string }}
          ref: main

  run_primary_account_enumeration:
    name: Run Recursive Enum (Primary Account)
    needs: [prepare_all_chunks_and_package, distribute_and_trigger_secondary]
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 && needs.distribute_and_trigger_secondary.outputs.primary_matrix_json != '[]'
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    strategy:
      fail-fast: false
      matrix:
        pair: ${{ fromJson(needs.distribute_and_trigger_secondary.outputs.primary_matrix_json) }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3
        with:
          # Fetch full Git history so previous commits are available for comparison
          fetch-depth: 0   
          
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.22'

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-
    
      - name: Download & Extract Chunks Package
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}
      - run: tar -xzvf *.tar.gz

      - name: Run Assetfinder with GNU Parallel on Chunk
        id: run_parallel
        shell: bash
        run: |
          
          CHUNK_FILE_PATH=${{ matrix.pair.chunk }}
          echo "Processing chunk '$CHUNK_FILE_PATH'..."
          if [ ! -f "$CHUNK_FILE_PATH" ]; then
            echo "ERROR: Chunk file '$CHUNK_FILE_PATH' not found!"
            
          fi
          
          SUB_CHUNK_DIR="sub-chunks"
          RAW_RESULTS_FILE="raw_subdomain_results.txt"
          
          # â”€â”€â”€ PREP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          # Ensure SUB_CHUNK_DIR is defined
         
          export SUB_CHUNK_DIR
          # Clean & recreate directories
          rm -rf "$SUB_CHUNK_DIR"
          mkdir -p "$SUB_CHUNK_DIR" "${SUB_CHUNK_DIR}/raw_results"
          
          TOTAL_LINES=$(wc -l < "$CHUNK_FILE_PATH")
          LINES_PER_SUB_CHUNK=$(awk -v t="$TOTAL_LINES" 'BEGIN { print int((t+4)/5) }')
          echo "WORKER: Main chunk has $TOTAL_LINES lines. Splitting into 5 sub-chunks of up to $LINES_PER_SUB_CHUNK lines each."
          
          split -l "$LINES_PER_SUB_CHUNK" "$CHUNK_FILE_PATH" "${SUB_CHUNK_DIR}/sub_chunk_"
                             
          parallel -j 5 'findomain --file "{}" --external-subdomains --quiet --unique-output "${SUB_CHUNK_DIR}/raw_results/result_{#}.txt"' ::: ${SUB_CHUNK_DIR}/sub_chunk_*
          
          # â”€â”€â”€ MERGE + DEDUPE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          cat "$SUB_CHUNK_DIR/raw_results"/result_*.txt | sort -u > "$RAW_RESULTS_FILE"
          echo "âœ… All done. Consolidated into $RAW_RESULTS_FILE with $(wc -l < "$RAW_RESULTS_FILE") subdomains"
         
      - name: Install Tools
        run: |
          # Installing smap
          if ! command -v smap >/dev/null; then
            echo "Installing smapâ€¦"
            go install -v github.com/s0md3v/smap/cmd/smap@latest
          else
            echo "smap already in cache"
          fi    
          # Installing inscope
          if ! command -v inscope >/dev/null; then
            echo "Installing inscopeâ€¦"
            go install -v github.com/tomnomnom/hacks/inscope@latest
          else
            echo "inscope already in cache"
          fi    
          
          if ! command -v anew >/dev/null; then
            echo "Installing anewâ€¦"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in cache"
          fi
          
          if ! command -v cut-cdn >/dev/null; then
            echo "Installing cut-cdnâ€¦"
            go install github.com/ImAyrix/cut-cdn@latest
          else
            echo "cut-cdn already in cache"
          fi     

          if ! command -v naabu >/dev/null; then
            echo "Installing naabuâ€¦"
            go install -v github.com/projectdiscovery/naabu/v2/cmd/naabu@latest
          else
            echo "naabu already in cache"
          fi

          pip3 install --no-cache-dir ipaddress
          
          echo "$HOME/go/bin" >> $GITHUB_PATH
          
      # ====================================================================
      # NEW STEP: PureDNS Resolution
      # ====================================================================
    
      - name: Resolve Discovered Subdomains with PureDNS
        id: run_puredns
        shell: bash
        run: |
                RAW_RESULTS_FILE="raw_subdomain_results.txt"
                PUREDNS_FILE="puredns_file.txt"
                RESOLVED_FILE="puredns_resolved.txt"
                TMP_CLEANMASSDNS=$(mktemp)
                MASSDNS="massdns.txt"
                #WILDCARD_FILE="wildcard_sub.txt"
                MASSDNS_FILE="massdns_file.txt"
                
                if [ ! -s "$RAW_RESULTS_FILE" ]; then
                        echo "INFO: Raw results file is empty. Nothing to resolve."
                        touch "$PUREDNS_FILE" # Create an empty file to prevent downstream errors
                        exit 0
                fi
                wget -qO resolvers.txt https://raw.githubusercontent.com/and0x00/resolvers.txt/refs/heads/main/resolvers.txt
                          
                echo "INFO: Resolving subdomains using puredns..."
                # The container image includes a list of public resolvers at /opt/resolvers.txt
                puredns resolve "$RAW_RESULTS_FILE" \
                  -r resolvers.txt \
                  --rate-limit 3000 \
                  --wildcard-tests 300 \
                  --wildcard-batch 100000 \
                  --rate-limit-trusted 2000 \
                  --write "$PUREDNS_FILE" \
                  --write-massdns "$MASSDNS" \
                  --quiet >/dev/null 2>&1
                                     
                echo "âœ… PureDNS resolution complete. $(wc -l < "$PUREDNS_FILE") subdomains were successfully resolved."
                echo "âœ… Downloading .scope file from recon-automation repo"
                wget -qO .scope https://raw.githubusercontent.com/Pcoder7/recon-automation/refs/heads/main/.scope 
                echo  " $(wc -l < .scope) lines of scope file is downloaded " 
                
                cat "$PUREDNS_FILE" | inscope -s .scope > "$RESOLVED_FILE" || true
                # echo "âœ… PureDNS resolution complete. $(wc -l < "$WILDCARD_FILE") Wildcard subdomains found."
                awk 'NF { sub(/\.$/,"",$1); print }' "$MASSDNS" > "$TMP_CLEANMASSDNS"
                awk ' \
                {gsub(/\r$/,"");sub(/^[ \t]+/,"");sub(/[ \t]+$/,"")} \
                FNR==NR{if($0)patterns[++c]=$0;next} \
                !setup{regex="";for(i=1;i<=c;i++){regex=regex (i>1?"|":"") "("patterns[i]")"};if(regex=="")regex="^\b$";setup=1} \
                $2=="A" && $1~regex \
                ' .scope "$TMP_CLEANMASSDNS" | anew -q "$MASSDNS_FILE"
                
                echo "âœ… PureDNS resolution complete. $(wc -l < "$MASSDNS_FILE")  massdns found."     
                rm -f "$TMP_CLEANMASSDNS" "$MASSDNS" "$PUREDNS_FILE"        
     
      - name: Sort Resolved Results into Root Domain Folders
        shell: bash
        run: |

                RESOLVED_FILE="puredns_resolved.txt"
                MASSDNS_FILE="massdns_file.txt"
                mkdir -p results
                # -------------------------
                # Option B: in-memory awk filter
                # Keep only massdns lines whose hostname (first field) exists in RESOLVED_FILE
                # -------------------------
                # If massdns file doesn't exist or is empty -> nothing to do
                if [ ! -s "$MASSDNS_FILE" ]; then
                        echo "INFO: No massdns file ($MASSDNS_FILE) present or empty; skipping filter."
                else
                        # If resolved file empty -> produce empty MASSDNS_FILE (no extraneous junk)
                        if [ ! -s "$RESOLVED_FILE" ]; then
                                echo "INFO: Resolved file is empty -> clearing $MASSDNS_FILE per policy."
                                : > "$MASSDNS_FILE"
                        else
                                echo "INFO: Filtering $MASSDNS_FILE using allowlist from $RESOLVED_FILE (in-memory awk)."
                                TMP_FILTERED=$(mktemp)
                                awk '
                                # First pass: read allowlist (RESOLVED_FILE) and use first field as hostname
                                FNR==NR {
                                        gsub(/\r/,""); sub(/^[ \t]+|[ \t]+$/,"");
                                        sub(/\.$/,"");
                                        host=$1;              # <-- use first field (hostname) not entire line
                                        host=tolower(host);
                                        if(length(host)) allowed[host]=1;
                                        next
                                }
                                # Second pass: process massdns lines (MASSDNS_FILE)
                                NF {
                                        host=$1; sub(/\.$/,"",host);
                                        if(allowed[tolower(host)]) print $0;
                                }' "$RESOLVED_FILE" "$MASSDNS_FILE" > "$TMP_FILTERED"
                                # atomically replace original massdns file with filtered one
                                mv "$TMP_FILTERED" "$MASSDNS_FILE"
                                echo "INFO: Filtered massdns count: $(wc -l < "$MASSDNS_FILE" || true)"
                        fi
                fi
                # -------------------------
                # Build combined list (same as before)
                # -------------------------
                # Create a combined list of all subdomains from both files to find all possible root domains
                cat "$RESOLVED_FILE" 2>/dev/null > combined_subdomains.txt
                awk '{print $1}' "$MASSDNS_FILE" 2>/dev/null >> combined_subdomains.txt
                if [ ! -s "combined_subdomains.txt" ]; then echo "INFO: No resolvable data in this chunk."; exit 0; fi
                # Extract unique root domains from the combined list
                dsieve -if "combined_subdomains.txt" -f 2 | sort -u > temp_root_domains.txt
                while read -r parent; do
                        if [ -z "$parent" ]; then continue; fi; mkdir -p "results/$parent"
                        # Logic for puredns results (only if the source file has content)
                        if [ -s "$RESOLVED_FILE" ]; then
                                grep -E "(^|\\.)${parent//./\\.}(\$)" --color=never "$RESOLVED_FILE" | anew -q "results/$parent/puredns_results.txt" || true
                        fi
                        # Logic for massdns file (only if the source file has content)
                        if [ -s "$MASSDNS_FILE" ]; then
                                grep -E "(^|\\.)${parent//./\\.}(\s|\$)" --color=never "$MASSDNS_FILE" | anew -q "results/$parent/massdns_file.txt" || true
                        fi
                done < temp_root_domains.txt
     
          
      - name: Compute SAFE_CHUNK (no slashes)
        run: |
          SAFE_CHUNK="${{ matrix.pair.chunk }}"
          SAFE_CHUNK="$(echo "$SAFE_CHUNK" | tr '/' '_')"
          echo "SAFE_CHUNK=$SAFE_CHUNK" >> $GITHUB_ENV    
          
      - name: Upload Primary Account Results
        uses: actions/upload-artifact@v4
        with:
          name: recon-results-primary-${{ env.SAFE_CHUNK }}
          path: results/
          retention-days: 1


  merge_results:
    name: Merge All Distributed Results
    runs-on: ubuntu-latest
    needs: run_primary_account_enumeration 
    if: always() 
    outputs:
      has_results: ${{ steps.check_artifacts.outputs.found }}    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-
         
      - name: Install Tools
        run: |
          # Installing anew
          if ! command -v anew >/dev/null; then
            echo "Installing anewâ€¦"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in cache"
          fi    
          
      - name: Download all result artifacts from all accounts
        uses: actions/download-artifact@v4
        with:
          # This pattern correctly finds artifacts from primary, secondary, and tertiary workers
          pattern: 'recon-results-*'
          path: temp-aggregated-results
          merge-multiple: true

      - name: Check if artifacts were downloaded
        id: check_artifacts
        shell: bash
        run: |
          if [ -d "temp-aggregated-results" ] && [ -n "$(ls -A temp-aggregated-results)" ]; then
            echo "-> Artifacts found. Proceeding with merge."
            echo "found=true" >> $GITHUB_OUTPUT
          else
            echo "-> No artifacts found to merge. Skipping the rest of this job."
            echo "found=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Consolidate all results into root domain folders
        id: consolidate
        run: |
          mkdir -p final_results
          
          # Aggregate puredns_results.txt files
          
          find temp-aggregated-results -type f -name "puredns_results.txt" | while read -r f; do D=$(basename "$(dirname "$f")"); mkdir -p "final_results/$D"; cat "$f" >> "final_results/$D/puredns_results.txt"; done
         
          # Aggregate massdns_file.txt files
          
          find temp-aggregated-results -type f -name "massdns_file.txt" | while read -r f; do D=$(basename "$(dirname "$f")"); mkdir -p "final_results/$D"; cat "$f" >> "final_results/$D/massdns_file.txt"; done
          
          # De-duplicate all aggregated files
          find final_results -type f -name "*.txt" -exec sort -u -o {} {} \;
          
         
          # Final guard clause to check if the process resulted in any actual data.
         
          if [ -z "$(ls -A final_results)" ]; then
            echo "::warning:: Result artifacts were downloaded, but they contained no valid data to merge."
            echo "has_results=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "has_results=true" >> $GITHUB_OUTPUT
          echo "âœ… Successfully consolidated results from all accounts."
          ls -R final_results

      - name: Upload Final Consolidated Artifact
        if: steps.consolidate.outputs.has_results == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-recon-results
          path: final_results/
          retention-days: 1

  commit_all_results:
    name: Commit All Results
    needs: merge_results 
    if: always() && needs.merge_results.outputs.has_results == 'true'
    runs-on: ubuntu-latest

    steps:
      - name: Download the single consolidated results artifact
        uses: actions/download-artifact@v4
        with:
          name: consolidated-recon-results
          path: final_results
  
      - name: Organize and Push to store-recon    
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
          STORE: ${{ secrets.STORE }}
          CORRELATION_ID: ${{ github.run_id }}
        run: |
          
          RESULTS_DIR="${GITHUB_WORKSPACE}/final_results"
          
          if [ ! -d "$RESULTS_DIR" ] || [ -z "$(ls -A "$RESULTS_DIR")" ]; then
            echo "::warning:: Results directory is empty or does not exist. Nothing to commit."
            exit 0
          fi
          
          echo "Cloning ${STORE} to commit results..."
          git config --global user.name "Assetfinder Bot"
          git config --global user.email "actions-bot@users.noreply.github.com"
          
          TMP_DIR="$(mktemp -d)"
          if ! git clone "https://x-access-token:${STORE_RECON_PAT}@github.com/${ACCOUNT2_USERNAME}/${STORE}.git" "$TMP_DIR"; then
            echo "::error:: Failed to clone the repository. Aborting the commit process."
            exit 0 # Exit successfully as requested
          fi
          cd "$TMP_DIR" 
          
          # --- Reusable function to merge artifact data ---
          run_merge() {
            echo "Merging new consolidated results into the repository..."
            for domain_dir in "${RESULTS_DIR}"/*; do
              if [ ! -d "$domain_dir" ]; then continue; fi
              domain_name=$(basename "$domain_dir")
              dest_repo_dir="results/$domain_name"
              mkdir -p "$dest_repo_dir"
              
              # MERGE BLOCK 1
              source_puredns_file="$domain_dir/puredns_results.txt"
              dest_all_subs_file="$dest_repo_dir/all_subdomains.txt"
              if [ -s "$source_puredns_file" ]; then
                <"$source_puredns_file" tr -d '\0' \
                  | grep '[[:alnum:]]' \
                  | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//' \
                  | sed -r "s/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g" \
                  > "$source_puredns_file.tmp" && mv "$source_puredns_file.tmp" "$source_puredns_file"
                  
                echo "  -> Merging puredns results into '$dest_all_subs_file'"
                temp_merged_file_1=$(mktemp)
                if [ -f "$dest_all_subs_file" ]; then cat "$source_puredns_file" "$dest_all_subs_file" | sort -u > "$temp_merged_file_1"; else sort -u "$source_puredns_file" > "$temp_merged_file_1"; fi
                mv "$temp_merged_file_1" "$dest_all_subs_file"
              fi
              
              # FIX: Added MERGE BLOCK 2 for massdns with full sanitization
              
              source_massdns_file="$domain_dir/massdns_file.txt"; 
              dest_massdns_file="$dest_repo_dir/massdns.txt"
              if [ -s "$source_massdns_file" ]; then
                <"$source_massdns_file" tr -d '\0'|grep '[[:alnum:]]'|sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//'|sed -r "s/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g" > tmp.txt && mv tmp.txt "$source_massdns_file"
                cat "$source_massdns_file" "$dest_massdns_file" 2>/dev/null | sort -u > tmp.txt && mv tmp.txt "$dest_massdns_file"
              fi
            done          

              
            
            # Stage all changes within the results directory
            git add results/
          }

          # --- Main Logic ---

          # 1. Perform the initial merge and check for changes
          run_merge
          if git diff --cached --quiet; then
            echo "No new unique data to commit in ${STORE}."
            exit 0
          fi
          
          # 2. Commit the changes locally
          echo "Committing changes locally..."
          git commit -m "feat: Add new assets from Recursive recon scan from Correlation ID: ${CORRELATION_ID}"
          
          # 3. Loop to sync and push the commit, with a robust retry mechanism
          MAX_ATTEMPTS=10
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
            echo "[Attempt $i/$MAX_ATTEMPTS] Pushing changes..."
            
            # Optimistic push first
            if git push -v origin main; then
              echo "âœ… Successfully pushed new consolidated results to ${STORE} on attempt $i."
              exit 0
            fi
            
            echo "::warning:: Push failed on attempt $i. Fetching latest changes from remote and re-applying local changes."
            
            # If push failed, fetch the latest state from the remote
            git fetch origin main
            if [ $? -ne 0 ]; then
                echo "::error:: Git fetch failed. Cannot safely retry."
                sleep $(( 5 * i ))
                continue # Try again
            fi
            
            # Reset local state to match remote, discarding the old local commit
            git reset --hard origin/main
            
            # Re-run the merge logic on top of the fresh, updated branch
            echo "Re-applying merge logic on top of the updated main branch..."
            run_merge
            
            # Check if there are still changes to commit after the re-merge
            if git diff --cached --quiet; then
              echo "No net new changes to commit after syncing with remote. Another run may have already pushed these results."
              exit 0
            fi
            
            # Re-commit the newly calculated changes
            echo "Re-committing changes for retry attempt..."
            git commit -m "feat: Add new assets from Recursive recon scan from Correlation ID: ${CORRELATION_ID} (retry)"

            sleep $(( 5 * i ))
          done

          echo "::error:: All $MAX_ATTEMPTS push attempts failed. The job will pass but the commit was NOT pushed."
          exit 0 
          
  trigger_port_scanning:
    name: Trigger Port Scanning Workflow
    needs: commit_all_results
    if: success()
    runs-on: ubuntu-latest
    steps:
      - name: Trigger next Rustyscan Workflow
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: rusty_scan.yml
          repo: Pcoder7/Rustyy_scan
          token: ${{ secrets.PAT_TOKEN }}
          ref: main
          inputs: >
            {
              "correlation_id": "${{ github.run_id }}",
              "results_repo_owner": "${{ env.ACCOUNT2_REPO_OWNER }}",
              "results_repo_name": "${{ env.STORE_REPO_NAME }}"
            }
          
