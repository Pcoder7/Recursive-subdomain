name: Distributed Recursive Enumeration

on:
  workflow_dispatch:

permissions:
  contents: write   # For creating matrix.json and artifacts
  actions: write    # For triggering workflows in other repositories

env:
  # --- Chunking and Distribution Configuration ---
  LINES_PER_CHUNK: 30             # MODIFIED: As requested for assetfinder seeds
  DISTRIBUTION_THRESHOLD: 20      # The minimum total chunks to trigger 3-way distribution
  PRIMARY_PERCENTAGE: 45          # 45% for the primary account
  SECONDARY_PERCENTAGE: 40        # 40% for the secondary account

  # --- Account 2 Details (for triggering and committing) ---
  ACCOUNT2_REPO_OWNER: ${{ secrets.ACCOUNT2_REPO_OWNER || 'pushrockzz' }}
  STORE_REPO_NAME: ${{ secrets.STORE }} # The 'store-recon' repo name

  # --- Account 3 Details ---
  ACCOUNT3_REPO_OWNER: ${{ secrets.ACCOUNT3_REPO_OWNER || 'Sari2705' }}
  ACCOUNT3_REPO_NAME: ${{ secrets.ACCOUNT3_REPO_NAME || 'puredns-resolve' }} # Or a dedicated repo

jobs:
  prepare_all_chunks_and_package:
    name: Prepare Seed Chunks & Package
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    outputs:
      all_chunks_matrix_json: ${{ steps.build_full_matrix.outputs.full_matrix }}
      total_chunks_count: ${{ steps.build_full_matrix.outputs.total_chunks }}
      chunk_package_artifact_name: "assetfinder-chunks-package-${{ github.run_id }}"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      # STEP 1: Build the smart seed list from the store-recon repo
      - name: Build Smart Tiered & Randomized Seed List
        id: create_final_list
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
          STORE: ${{secrets.STORE}}# PAT with access to the store-recon repo
        run: |
          set -e
          echo "INFO: Performing a fast, shallow clone of the results repository..."
          git clone --depth 1 "https://x-access-token:${{ env.STORE_RECON_PAT }}@github.com/${{ env.ACCOUNT2_REPO_OWNER }}/${{ env.STORE_REPO_NAME }}.git" store-recon-temp
          
          echo "INFO: Aggregating all results into a master list..."
          find store-recon-temp/results -type f -name "puredns_result.txt" -exec awk '{print $1}' {} + > all-resolved-subdomains.txt
          if [ ! -s all-resolved-subdomains.txt ]; then
              echo "::warning:: Master subdomain list is empty. No seeds to generate."
              touch final-subdomain-list.txt
              exit 0
          fi
          echo "INFO: Extracting unique root domains..."
          dsieve -if all-resolved-subdomains.txt -f 2 | sort -u > unique_root_domains.txt
          > final-subdomain-list.txt
          echo "INFO: Building smart seed list for each root domain..."
          while read -r ROOT_DOMAIN; do
            if [ -z "$ROOT_DOMAIN" ]; then continue; fi
            echo "====================================================="
            echo "Processing Root Domain: $ROOT_DOMAIN"
            
            TEMP_ROOT_LIST="${TMPDIR}/${ROOT_DOMAIN}-list.txt"
            grep -E "(^|\\.)${ROOT_DOMAIN//./\\.}(\$)" all-resolved-subdomains.txt > "$TEMP_ROOT_LIST"
            if [ ! -s "$TEMP_ROOT_LIST" ]; then continue; fi
            TEMP_SEED_LIST_DOMAIN="${TMPDIR}/${ROOT_DOMAIN}-seeds.txt"
            > "$TEMP_SEED_LIST_DOMAIN"
            
            # Tier 1: Random sample of the top-tier
            echo "  -> Getting a random 250 sample from the top 500 subdomains..."
            dsieve -if "$TEMP_ROOT_LIST" -f 3 -top 500 | shuf | head -n 250 | anew -q "$TEMP_SEED_LIST_DOMAIN"
            
            # Tier 2: Random sample of the mid-tier
            echo "  -> Getting a random 150 sample from the mid-tier (ranks 501-1000)..."
            dsieve -if "$TEMP_ROOT_LIST" -f 3 -top 1000 | tail -n 500 | shuf | head -n 150 | anew -q "$TEMP_SEED_LIST_DOMAIN"
            cat "$TEMP_SEED_LIST_DOMAIN" >> final-subdomain-list.txt
          done < unique_root_domains.txt
          
          echo "INFO: Performing final de-duplication of the master seed list..."
          mv final-subdomain-list.txt final-subdomain-list.tmp
          sort -u final-subdomain-list.tmp > final-subdomain-list.txt
          rm final-subdomain-list.tmp
          echo "SUCCESS: 'final-subdomain-list.txt' created with $(wc -l < final-subdomain-list.txt) total unique seeds."
          rm -rf store-recon-temp
      # STEP 2: Chunk the final seed list for distribution
      - name: Build Full Matrix & Create Chunks
        id: build_full_matrix
        shell: bash
        run: |
          JSON_MATRIX='[]'
          if [ ! -s final-subdomain-list.txt ]; then
            echo "WARNING: final-subdomain-list.txt is empty. No chunks will be generated."
          else
            echo "INFO: Creating chunk directory 'chunks/all-seeds'..."
            mkdir -p chunks/all-seeds
            
            echo "INFO: Splitting 'final-subdomain-list.txt' into chunks of $LINES_PER_CHUNK lines..."
            split -l "$LINES_PER_CHUNK" -a 4 --numeric-suffixes=1 final-subdomain-list.txt "chunks/all-seeds/chunk_"
            
            while IFS= read -r chunk_file_path; do
              JSON_MATRIX=$(printf '%s' "$JSON_MATRIX" | jq -c --arg c "$chunk_file_path" '. + [{chunk:$c}]')
            done < <(find "chunks/all-seeds/" -name 'chunk_*' -type f)
          fi
          
          TOTAL_CHUNKS=$(echo "$JSON_MATRIX" | jq 'length')
          echo "FINAL_BUILD_INFO: Total chunks in matrix: $TOTAL_CHUNKS"
          echo "$JSON_MATRIX" > full_matrix.json
          echo "full_matrix=$JSON_MATRIX" >> $GITHUB_OUTPUT
          echo "total_chunks=$TOTAL_CHUNKS" >> $GITHUB_OUTPUT
      # STEP 3: Package the chunks into an artifact
      - name: Package All Chunks
        id: package_chunks
        if: steps.build_full_matrix.outputs.total_chunks > 0
        shell: bash
        run: |
          BASE_ARTIFACT_NAME="assetfinder-chunks-package-${{ github.run_id }}"
          PACKAGE_TAR_FILENAME="${BASE_ARTIFACT_NAME}.tar.gz"
          echo "INFO: Creating tarball: $PACKAGE_TAR_FILENAME"
          tar -czvf "$PACKAGE_TAR_FILENAME" chunks full_matrix.json
          echo "package_tar_filename=$PACKAGE_TAR_FILENAME" >> $GITHUB_OUTPUT
          echo "base_artifact_name=$BASE_ARTIFACT_NAME" >> $GITHUB_OUTPUT
      # STEP 4: Upload the artifact
      - name: Upload Full Chunks Package
        if: steps.build_full_matrix.outputs.total_chunks > 0
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.package_chunks.outputs.base_artifact_name }}
          path: ${{ steps.package_chunks.outputs.package_tar_filename }}
          retention-days: 1

  # This job is identical to your reference and needs no changes
  distribute_and_trigger_secondary:
    name: Distribute Work & Trigger Workers
    needs: prepare_all_chunks_and_package
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0
    runs-on: ubuntu-latest
    outputs:
      primary_matrix_json: ${{ steps.calculate_distribution.outputs.primary_matrix }}
      secondary_matrix_json: ${{ steps.calculate_distribution.outputs.secondary_matrix }}
      tertiary_matrix_json: ${{ steps.calculate_distribution.outputs.tertiary_matrix }}
    steps:
      - name: Calculate Chunk Distribution for Accounts
        id: calculate_distribution
        shell: bash
        run: |
          ALL_CHUNKS_JSON='${{ needs.prepare_all_chunks_and_package.outputs.all_chunks_matrix_json }}'
          TOTAL_CHUNKS=${{ needs.prepare_all_chunks_and_package.outputs.total_chunks_count }}
          THRESHOLD=${{ env.DISTRIBUTION_THRESHOLD }}
          if [ "$TOTAL_CHUNKS" -lt "$THRESHOLD" ]; then
            PRIMARY_CHUNKS_JSON="$ALL_CHUNKS_JSON"
            SECONDARY_CHUNKS_JSON="[]"
            TERTIARY_CHUNKS_JSON="[]"
          else
            PRIMARY_PERCENT=${{ env.PRIMARY_PERCENTAGE }}
            SECONDARY_PERCENT=${{ env.SECONDARY_PERCENTAGE }}
            PRIMARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $PRIMARY_PERCENT) / 100" | bc)
            SECONDARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $SECONDARY_PERCENT) / 100" | bc)
            OFFSET_FOR_TERTIARY=$((PRIMARY_CHUNK_COUNT + SECONDARY_CHUNK_COUNT))
            PRIMARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson limit "$PRIMARY_CHUNK_COUNT" '.[0:$limit]')
            SECONDARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$PRIMARY_CHUNK_COUNT" --argjson limit "$SECONDARY_CHUNK_COUNT" '.[$offset : $offset+$limit]')
            TERTIARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$OFFSET_FOR_TERTIARY" '.[$offset:]')
          fi
          
          echo "primary_matrix=$PRIMARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
          echo "secondary_matrix=$SECONDARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
          echo "tertiary_matrix=$TERTIARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
      
      # The rest of this job (preparing payloads and triggering) is complex but can be copied
      # verbatim from your original workflow. Just ensure the 'workflow:' name is correct.
      # For brevity, I'll show one trigger block. Repeat for tertiary.
      - name: Prepare Trigger Payload For Secondary
        id: prepare_payload_secondary
        if: steps.calculate_distribution.outputs.secondary_matrix != '[]'
        shell: bash
        run: |
          MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.secondary_matrix }}'
          JSON_PAYLOAD=$(jq -cn \
            --arg server_url "${{ github.server_url }}" \
            --arg repo_owner "${{ github.repository_owner }}" \
            --arg repo_name "${{ github.event.repository.name }}" \
            --arg run_id "${{ github.run_id }}" \
            --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
            --arg matrix_as_a_string "${MATRIX_AS_STRING}" \
            '{
              "primary_github_server_url": $server_url,
              "primary_repo_owner": $repo_owner,
              "primary_repo_name": $repo_name,
              "primary_run_id": $run_id,
              "chunk_package_artifact_name": $artifact_name,
              "secondary_matrix_json": $matrix_as_a_string
            }')
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT
      - name: Trigger Secondary Account Workflow
        if: steps.calculate_distribution.outputs.secondary_matrix != '[]'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: run-assetfinder.yml # IMPORTANT: This must match the workflow name in the secondary repo
          repo: ${{ secrets.ACCOUNT2_REPO_OWNER }}/${{ secrets.STORE }} # Assuming worker workflow is in store-recon repo
          token: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          inputs: ${{ steps.prepare_payload_secondary.outputs.json_string }}
          ref: main
      
      # ... (Add identical 'Prepare' and 'Trigger' blocks for Tertiary Account) ...
      #====================================================================
      # ADD THE FIRST SLEEP STEP HERE
      # ====================================================================
      #- name: Add delay before triggering Tertiary Trigger worker
       # if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        #run: |
         # echo "Pausing for 30 seconds to space out worker triggers and avoid platform throttling..."
          #sleep 30

      - name: Sleep for 20 seconds
        uses: jakejarvis/wait-action@master
        with:
          time: '20s'

      # ====================================================================
      
      - name: Prepare Trigger Payload For Tertiary
        # This is a NEW step, a copy of the secondary payload step for Account 3.
        id: prepare_payload_tertiary
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        shell: bash
        run: |
          MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.tertiary_matrix }}'
          JSON_PAYLOAD=$(jq -cn \
            --arg run_id "${{ github.run_id }}" \
            '{
              "primary_run_id": $run_id
            }')
             echo "Constructed JSON Payload: $JSON_PAYLOAD"
             echo "json_string=$JSON_PAYLOAD" >> "$GITHUB_OUTPUT"
           
           JSON_PAYLOAD=$(jq -cn \
             --arg server_url "${{ github.server_url }}" \
             --arg repo_owner "${{ github.repository_owner }}" \
             --arg repo_name "${{ github.event.repository.name }}" \
             --arg run_id "${{ github.run_id }}" \
             --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
             --arg matrix_as_a_string "${MATRIX_AS_STRING}" \
             '{
               "primary_github_server_url": $server_url,
               "primary_repo_owner": $repo_owner,
               "primary_repo_name": $repo_name,
               "primary_run_id": $run_id,
               "chunk_package_artifact_name": $artifact_name,
               "tertiary_matrix_json": $matrix_as_a_string 
             }')
          
          echo "Constructed JSON Payload: $JSON_PAYLOAD"
             # Set the constructed JSON string as an output of this step
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT
      - name: DEBUG - Show Tertiary Inputs String
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        run: |
          echo "DEBUG: Inputs for TERTIARY account:"
          # echo "${{ steps.prepare_payload_tertiary.outputs.json_string }}"    
      - name: Trigger Tertiary Account Workflow
        # This is a NEW step, a copy of the secondary trigger step for Account 3.
        id: trigger_tertiary
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: resolve.yml
          repo: ${{ secrets.ACCOUNT3_REPO_OWNER }}/${{ secrets.ACCOUNT3_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_TERTIARY_ACCOUNT_REPO }}
          inputs: ${{ steps.prepare_payload_tertiary.outputs.json_string }}
          ref: main

  run_primary_account_enumeration:
    name: Run Recursive Enum (Primary Account)
    needs: [prepare_all_chunks_and_package, distribute_and_trigger_secondary]
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 && needs.distribute_and_trigger_secondary.outputs.primary_matrix_json != '[]'
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    strategy:
      fail-fast: false
      matrix:
        pair: ${{ fromJson(needs.distribute_and_trigger_secondary.outputs.primary_matrix_json) }}
    steps:
      - name: Download & Extract Chunks Package
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}
      - run: tar -xzvf *.tar.gz

      - name: Run Assetfinder with GNU Parallel on Chunk
        id: run_parallel
        shell: bash
        run: |
          set -euo pipefail        
          
          CHUNK_FILE_PATH=${{ matrix.pair.chunk }}
          echo "Processing chunk '$CHUNK_FILE_PATH'..."
          if [ ! -f "$CHUNK_FILE_PATH" ]; then
            echo "ERROR: Chunk file '$CHUNK_FILE_PATH' not found!"
            
          fi
          
          SUB_CHUNK_DIR="sub-chunks"
          RAW_RESULTS_FILE="raw_subdomain_results.txt"
          
          # ─── PREP ──────────────────────────────────────────────────────────────────
          # Ensure SUB_CHUNK_DIR is defined
         
          export SUB_CHUNK_DIR
          # Clean & recreate directories
          rm -rf "$SUB_CHUNK_DIR"
          mkdir -p "$SUB_CHUNK_DIR" "${SUB_CHUNK_DIR}/raw_results"
          
          TOTAL_LINES=$(wc -l < "$CHUNK_FILE_PATH")
          LINES_PER_SUB_CHUNK=$(awk -v t="$TOTAL_LINES" 'BEGIN { print int((t+4)/5) }')
          echo "WORKER: Main chunk has $TOTAL_LINES lines. Splitting into 5 sub-chunks of up to $LINES_PER_SUB_CHUNK lines each."
          
          split -l "$LINES_PER_SUB_CHUNK" "$CHUNK_FILE_PATH" "${SUB_CHUNK_DIR}/sub_chunk_"
                   
          parallel -j 5 'findomain --file "{}" --external-subdomains --quiet --unique-output "${SUB_CHUNK_DIR}/raw_results/result_{#}.txt"' ::: ${SUB_CHUNK_DIR}/sub_chunk_*
          
          # ─── MERGE + DEDUPE ─────────────────────────────────────────────────────────
          cat "$SUB_CHUNK_DIR/raw_results"/result_*.txt | sort -u > "$RAW_RESULTS_FILE"
          echo "✅ All done. Consolidated into $RAW_RESULTS_FILE with $(wc -l < "$RAW_RESULTS_FILE") subdomains"
         
          
      # ====================================================================
      # NEW STEP: PureDNS Resolution
      # ====================================================================
      - name: Resolve Discovered Subdomains with PureDNS
        id: run_puredns
        shell: bash
        run: |
          RAW_RESULTS_FILE="raw_subdomain_results.txt"
          RESOLVED_FILE="puredns_resolved.txt"
          WILDCARD_FILE="wildcard_sub.txt"
          MASSDNS_FILE="massdns_file.txt"
          
          if [ ! -s "$RAW_RESULTS_FILE" ]; then
            echo "INFO: Raw results file is empty. Nothing to resolve."
            touch "$RESOLVED_FILE" # Create an empty file to prevent downstream errors
            exit 0
          fi

          wget -qO resolvers.txt https://raw.githubusercontent.com/and0x00/resolvers.txt/refs/heads/main/resolvers.txt
          
          echo "INFO: Resolving subdomains using puredns..."
          # The container image includes a list of public resolvers at /opt/resolvers.txt
          puredns resolve "$RAW_RESULTS_FILE" \
            -r resolvers.txt \
            --rate-limit 3000 \
            --skip-validation \
            --write "$RESOLVED_FILE" \
            --write-wildcards "$WILDCARD_FILE" \
            --write-massdns "$MASSDNS_FILE" \
            --wildcard-batch 100000 --wildcard-tests 250 --quiet >/dev/null 2>&1
            
          
          echo "✅ PureDNS resolution complete. $(wc -l < "$RESOLVED_FILE") subdomains were successfully resolved."

          echo "✅ PureDNS resolution complete. $(wc -l < "$WILDCARD_FILE") Wildcard subdomains found."

          cat "$MASSDNS_FILE" | head -n15 
          
          
      # ====================================================================
      # MOVED AND MODIFIED STEP: Sorting
      # ====================================================================
      - name: Sort Resolved Results into Root Domain Folders
        shell: bash
        run: |
          # This step now uses the output from puredns as its source
          RESOLVED_FILE="puredns_resolved.txt"
          WILDCARD_FILE="wildcard_sub.txt"
          MASSDNS_FILE="massdns_file.txt"
          echo "INFO: Sorting resolved results into root-domain specific files..."
          mkdir -p results 
          
          if [ ! -s "$RESOLVED_FILE" ]; then
            echo "INFO: PureDNS found no resolvable domains in this chunk."
            exit 0
          fi
          
          # First, find which root domains are present in our *resolved* results
          dsieve -if "$RESOLVED_FILE" -f 2 | sort -u > temp_root_domains.txt
          
          # Now, loop through those root domains and extract their subdomains
          while read -r parent; do
            if [ -z "$parent" ]; then continue; fi
            mkdir -p "results/$parent"
            outfile="results/$parent/puredns_results.txt" # This name is kept for compatibility with commit job
            echo "  -> Filtering results for '$parent' into '$outfile'"
            # CRITICAL: Grep from the RESOLVED_FILE, not the raw results
            grep -E "(^|\\.)${parent//./\\.}(\$)" "$RESOLVED_FILE" | anew -q "$outfile"
          done < temp_root_domains.txt
            
      - name: Compute SAFE_CHUNK (no slashes)
        run: |
          SAFE_CHUNK="${{ matrix.pair.chunk }}"
          SAFE_CHUNK="$(echo "$SAFE_CHUNK" | tr '/' '_')"
          echo "SAFE_CHUNK=$SAFE_CHUNK" >> $GITHUB_ENV    
          
      - name: Upload Primary Account Results
        uses: actions/upload-artifact@v4
        with:
          name: recon-results-primary-${{ env.SAFE_CHUNK }}
          path: results/
          retention-days: 1

# ====================================================================
# This job remains unchanged and is already correctly configured
# to consume the artifacts produced by the puredns-enabled worker.
# ====================================================================
  merge_results:
    name: Merge All Distributed Results
    runs-on: ubuntu-latest
    # This should be triggered after all worker workflows have had time to complete.
    # In a single-workflow setup, it would need `run_primary_account_enumeration`.
    # In a multi-workflow setup, it would be triggered via `repository_dispatch`.
    needs: run_primary_account_enumeration 
    if: always() 
    outputs:
      has_results: ${{ steps.consolidate.outputs.has_results }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-
         
      - name: Install Tools
        run: |
          # Installing anew
          if ! command -v anew >/dev/null; then
            echo "Installing anew…"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in cache"
          fi    
          
      - name: Download all result artifacts from all accounts
        uses: actions/download-artifact@v4
        with:
          # This pattern correctly finds artifacts from primary, secondary, and tertiary workers
          pattern: 'recon-results-*'
          path: temp-aggregated-results
          merge-multiple: true

      - name: Consolidate all results into root domain folders
        id: consolidate
        shell: bash
        run: |
          set -e
          mkdir -p final_results
          
          if [ ! -d "temp-aggregated-results" ] || [ -z "$(ls -A temp-aggregated-results)" ]; then
            echo "::warning:: No result artifacts were found from any account. Nothing to merge."
            echo "has_results=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "INFO: Aggregating all downloaded (and puredns-resolved) results..."
          
          # This 'find' command works perfectly because the worker job still outputs
          # a file named 'resolved_host.txt' in the correct directory structure.
          find temp-aggregated-results -type f -name "puredns_results.txt" | while read -r filepath; do
            parent_domain=$(basename "$(dirname "$filepath")")
            mkdir -p "final_results/$parent_domain"
            cat "$filepath" | anew -q "final_results/$parent_domain/puredns_result.txt"
          done
          
          if [ -z "$(ls -A final_results)" ]; then
            echo "::warning:: Result artifacts were downloaded, but they contained no resolved domains."
            echo "has_results=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "has_results=true" >> $GITHUB_OUTPUT
          echo "✅ Successfully consolidated results from all accounts."
          ls -R final_results

      - name: Upload Final Consolidated Artifact
        if: steps.consolidate.outputs.has_results == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-recon-results
          path: final_results/
          retention-days: 1

# ====================================================================
# This job also remains unchanged. It correctly consumes the artifact
# from the merge_results job.
# ====================================================================
  commit_all_results:
    name: Commit All Results
    needs: merge_results 
    if: needs.merge_results.outputs.has_results == 'true'
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    steps:
      - name: Download the single consolidated results artifact
        uses: actions/download-artifact@v4
        with:
          name: consolidated-recon-results
          path: final_results

      - name: Organize and Push to store-recon
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
          STORE: ${{ secrets.STORE }}
        run: |
          set -e
          
          echo "Cloning store-recon to commit results..."
          git config --global user.name "Assetfinder Bot"
          git config --global user.email "actions-bot@users.noreply.github.com"
          
          TMP_DIR="$(mktemp -d)"
          git clone "https://x-access-token:${STORE_RECON_PAT}@github.com/${ACCOUNT2_USERNAME}/${STORE}.git" "$TMP_DIR"
          cd "$TMP_DIR" || exit 1
          
          echo "Merging new consolidated results into the repository..."
          for domain_dir in ../final_results/*; do
            domain_name=$(basename "$domain_dir")
            source_file="$domain_dir/puredns_results.txt"
            dest_dir="results/$domain_name"
            dest_file="$dest_dir/puredns_results.txt"
            
            mkdir -p "$dest_dir"
            
            temp_merged_file=$(mktemp)
            if [ -f "$dest_file" ]; then
              cat "$source_file" "$dest_file" | sort -u > "$temp_merged_file"
            else
              sort -u "$source_file" > "$temp_merged_file"
            fi
            mv "$temp_merged_file" "$dest_file"
          done
          
          if git diff --cached --quiet && git diff --quiet; then
            echo "No changes to commit in store-recon."
            exit 0
          fi
          
          git add results/
          git commit -m "feat: Add new subdomains from distributed recon scan (Run ID ${{ github.run_id }})"
          git push origin main
          echo "✅ Successfully pushed new consolidated results to store-recon."
